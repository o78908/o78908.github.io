[{"section":"Blog","slug":"/blog/2024/marketing_ds/","title":"Marketing Data Science","description":"123","date":"July 29, 2024","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/article_picture/dataAnalytics_hubd510a47a29ee5300d2a8fbe6dddf091_571474_420x0_resize_q80_h2_lanczos.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"420\"\n          height=\"280\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/article_picture\\/dataAnalytics_hubd510a47a29ee5300d2a8fbe6dddf091_571474_420x0_resize_q80_lanczos.jpg';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","imageSM":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n          \n          \n          \n          \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/article_picture/dataAnalytics_hubd510a47a29ee5300d2a8fbe6dddf091_571474_100x100_fill_q80_h2_lanczos_smart1.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"100\"\n          height=\"100\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/article_picture\\/dataAnalytics_hubd510a47a29ee5300d2a8fbe6dddf091_571474_100x100_fill_q80_lanczos_smart1.jpg';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","searchKeyword":"","categories":"Application, DataAnalytics","tags":"Application, DataAnalytics, 2024","content":"20240221 (1)Service Quality Scale (SERVQUAL) Parasuraman,A.;Berry,Leonard L.;Zeithaml,Valarie A., “SERVQUAL: A Multiple-Item Scale For Measuring Consumer Perceptions of Service Quality”, Journal of Retailing, 1988, 64, 1, 12-40.\n服務品質衡量量表為SERVQUAL和SERVPERF最為普遍。兩者當中，又以SERVQUAL量表最常被使用。 SERVQUAL量表是由Parasuraman, Berry和Zeitham三位學者於1988年提出，用於衡量服務品質，也是後續許多學者研究基礎。\n針對服務品質概論(PZB模式)中十種屬性加以抽樣、驗證及重新定義成五個構面： 可靠性(Reliability)是能夠準確可靠地執行承諾的服務； 反應性(Responsiveness)是願意幫助顧客並即時提供服務； 同理心(Empathy)是關注給予客戶關懷和客製化服務； 保證性(Assurance)是員工的知識和禮貌及使人產生信任和信心的能力； 有形性(Tangibility)是實際設施、旅店設備以及服務人員等。\n(2) 服務品質網路問卷設計，範例 https://forms.gle/FtoEuFZGsyxzwqt37 https://forms.gle/ZdHJeuh2xYm2J7YG6 圖書館服務品質問卷範例 LibQual\n旅宿業館服務品質問卷範例 LQI\n(3) Importance - Performacne Analysis Martilla, J.A. and James, J.C. (1977) Importance-Performance Analysis. Journal of Marketing, 41, 77-79. 如何分析 ? ( 範例NCHUMTSQ_OK.xlsx)\n本週資料集請點我 python程式碼 import pandas as pd import numpy as np from plotnine import * import matplotlib.pyplot as plt # 讀取 Excel 文件 data = pd.read_excel(\u0026#34;NCHUMTSQ.xlsx\u0026#34;, sheet_name=0) # 提取奇數列和偶數列數據 col_odd = np.arange(len(data.columns)) % 2 == 0 data_col_odd = data.iloc[:, col_odd] data_col_satisfaction = data_col_odd.iloc[:, 1:] data_col_importance = data.iloc[:, ~col_odd] # Calculate the mean of each column in the satisfaction DataFrame satisfaction_df_mean = data_col_satisfaction.mean(axis=0) # Slice the means for different dimensions satisfaction_df_Tangibility = satisfaction_df_mean[0:4] satisfaction_df_Assurance = satisfaction_df_mean[4:9] satisfaction_df_Empathy = satisfaction_df_mean[9:14] satisfaction_df_Responsiveness = satisfaction_df_mean[14:18] satisfaction_df_Reliability = satisfaction_df_mean[18:22] # Calculate the mean for each dimension satisfaction_df_dim = { \u0026#39;Tangibility\u0026#39;: satisfaction_df_Tangibility.mean(), \u0026#39;Assurance\u0026#39;: satisfaction_df_Assurance.mean(), \u0026#39;Empathy\u0026#39;: satisfaction_df_Empathy.mean(), \u0026#39;Responsiveness\u0026#39;: satisfaction_df_Responsiveness.mean(), \u0026#39;Reliability\u0026#39;: satisfaction_df_Reliability.mean() } # Calculate the mean of each column in the importance DataFrame importance_df_mean = data_col_importance.mean(axis=0) # Slice the means for different dimensions importance_df_Tangibility = importance_df_mean[0:4] importance_df_Assurance = importance_df_mean[4:9] importance_df_Empathy = importance_df_mean[9:14] importance_df_Responsiveness = importance_df_mean[14:18] importance_df_Reliability = importance_df_mean[18:22] # Calculate the mean for each dimension importance_df_dim = { \u0026#39;Tangibility\u0026#39;: importance_df_Tangibility.mean(), \u0026#39;Assurance\u0026#39;: importance_df_Assurance.mean(), \u0026#39;Empathy\u0026#39;: importance_df_Empathy.mean(), \u0026#39;Responsiveness\u0026#39;: importance_df_Responsiveness.mean(), \u0026#39;Reliability\u0026#39;: importance_df_Reliability.mean() } Attribute_df_dim = { \u0026#39;Tangibility\u0026#39;: \u0026#39;Tangibility\u0026#39;, \u0026#39;Assurance\u0026#39;: \u0026#39;Assurance\u0026#39;, \u0026#39;Empathy\u0026#39;: \u0026#39;Empathy\u0026#39;, \u0026#39;Responsiveness\u0026#39;: \u0026#39;Responsiveness\u0026#39;, \u0026#39;Reliability\u0026#39;: \u0026#39;Reliability\u0026#39; } # Print the results print(satisfaction_df_dim) print(importance_df_dim) # 創建 IPA 數據框 #ipa_df = pd.DataFrame({\u0026#39;importance_df_dim\u0026#39;: importance_df_dim, \u0026#39;satisfaction_df_dim\u0026#39;: satisfaction_df_dim}) df = pd.DataFrame( { \u0026#39;Attribute\u0026#39;: Attribute_df_dim, \u0026#39;Importance\u0026#39;: importance_df_dim, \u0026#39;Performance\u0026#39;: satisfaction_df_dim}) # Plot fig, ax = plt.subplots() #df.plot(kind=\u0026#39;scatter\u0026#39;, x=\u0026#39;Importance\u0026#39;, y=\u0026#39;Performance\u0026#39;, ax=ax) df.plot(kind=\u0026#39;scatter\u0026#39;, x=\u0026#39;Performance\u0026#39;, y=\u0026#39;Importance\u0026#39;, ax=ax) # Label points in the scatter plot for i, txt in enumerate(df.Attribute): ax.annotate(txt, (df.Performance[i],df.Importance[i] )) # Set chart title and labels ax.set_title(\u0026#39;Importance-Performance Analysis\u0026#39;) ax.set_xlabel(\u0026#39;Performance\u0026#39;) ax.set_ylabel(\u0026#39;Importance\u0026#39;) # Draw mean lines for importance and performance mean_importance = df[\u0026#39;Importance\u0026#39;].mean() mean_performance = df[\u0026#39;Performance\u0026#39;].mean() ax.axvline(x=mean_performance, coloR\u0026#39;r\u0026#39;, linestyle=\u0026#39;--\u0026#39;) ax.axhline(y=mean_importance, coloR\u0026#39;r\u0026#39;, linestyle=\u0026#39;--\u0026#39;) # Show plot plt.show() R語言程式碼 #----------------------------------------------- #install packages required #load package via library() #----------------------------------------------- library(xlsx) library(dplyr) library(webshot) library(htmlwidgets) library(igraph) library(ggraph) library(widyr) library(ggplot2) library(ggrepel) library(plotly) library(showtext) #------------set working directory ------------ #setwd(\u0026#34;c://Koong//csv//SERVQUAL_IPA\u0026#34;) #----------------------------------------------- #-----Read the first worksheet in the file input.xlsx. data \u0026lt;- read.xlsx(\u0026#34;NCHUMTSQ.xlsx\u0026#34;, sheetIndex = 1) #--------------------------------------------------- #head(data) #head(data[1:3,1:2]) #colnames(data)[2] #--------------------------------------------------- col_odd \u0026lt;- seq_len(ncol(data)) %% 2 # Create column indicator col_odd # Print column indicator #oddfiled \u0026lt;- col_odd == 1 data_col_odd \u0026lt;- data[ , col_odd == 1] # Subset odd columns data_col_odd # Print odd columns data_col_satisfaction \u0026lt;- data_col_odd[,-1] data_col_satisfaction data_col_importance \u0026lt;- data[ , col_odd == 0] # Subset even columns data_col_importance # Print even columns satisfaction_df \u0026lt;- data.frame(data_col_satisfaction) satisfaction_df_mean \u0026lt;- colMeans(satisfaction_df) #satisfaction_df_mean \u0026lt;- colMeans(satisfaction_df[sapply(satisfaction_df, is.numeric)]) satisfaction_df_Tangibility \u0026lt;- satisfaction_df_mean[1:4] satisfaction_df_Assurance \u0026lt;- satisfaction_df_mean[5:9] satisfaction_df_Empathy \u0026lt;- satisfaction_df_mean[10:14] satisfaction_df_Responseness \u0026lt;- satisfaction_df_mean[15:18] satisfaction_df_Reliability \u0026lt;- satisfaction_df_mean[19:22] satisfaction_df_dim \u0026lt;- c(Tangibility=mean(satisfaction_df_Tangibility), Assurance=mean(satisfaction_df_Assurance), Empathy=mean(satisfaction_df_Empathy), Responseness=mean(satisfaction_df_Responseness), Reliability=mean(satisfaction_df_Reliability)) #-------Commonly used statstics function ------------------- #mean(duration) #median(duration) #quantile(duration) #boxplot(duration, horizontal=TRUE) #var(duration) #sd(duration) #https://methodenlehre.github.io/SGSCLM-R-course/index.html #---------------------------------------------------------- importance_df \u0026lt;- data.frame(data_col_importance) importance_df_mean \u0026lt;- colMeans(importance_df) #importance_df_mean \u0026lt;- colMeans(importance_df[sapply(importance_df, is.numeric)]) importance_df_Tangibility \u0026lt;- importance_df_mean[1:4] importance_df_Assurance \u0026lt;- importance_df_mean[5:9] importance_df_Empathy \u0026lt;- importance_df_mean[10:14] importance_df_Responseness \u0026lt;- importance_df_mean[15:18] importance_df_Reliability \u0026lt;- importance_df_mean[19:22] importance_df_dim \u0026lt;- c(Tangibility=mean(importance_df_Tangibility), Assurance=mean(importance_df_Assurance), Empathy=mean(importance_df_Empathy), Responseness=mean(importance_df_Responseness), Reliability=mean(importance_df_Reliability)) #--------------------------------------------------- satisfaction_df_dim importance_df_dim #--------------------------------------------------- ipa_df \u0026lt;- data.frame(importance_df_dim,satisfaction_df_dim) ipa_df ipa \u0026lt;- NULL ipa \u0026lt;- ipa_df %\u0026gt;% mutate( cmove = importance_df_dim - mean(importance_df_dim) ) %\u0026gt;% mutate( smove = satisfaction_df_dim - mean(satisfaction_df_dim )) %\u0026gt;% data.frame() #--------IPA1 GGplot--------------------------------------------------------- empty_theme \u0026lt;- theme( plot.background = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.border = element_blank(), panel.background = element_blank(), axis.line = element_blank(), axis.ticks = element_blank(), axis.text.y = element_text(angle = 90)) #------------------------------------------------ #ggplot(\u0026lt;????\u0026gt;)+ aes(\u0026lt;?y?й?��\u0026gt;) #?y?й?�� # ?]?m?e?? # geom_\u0026lt;?X???ϥ?????\u0026gt;( # ?e?W?X???ϥ? # https://ggplot2-book.org/index.html #------------------------------------------------ ggplot(ipa, aes(x = smove, y = cmove, label = row.names(ipa))) + empty_theme + theme(panel.border = element_rect(colour = \u0026#34;lightgrey\u0026#34;, fill=NA, size=1))+ labs(title = \u0026#34;IPA analysis\u0026#34;, y = \u0026#34;Counts of Dimension Sentences\u0026#34;, x = \u0026#34;Sentiment Score of Dimension\u0026#34;) + geom_vline(xintercept = 0, colour = \u0026#34;lightgrey\u0026#34;, size=0.5) + geom_hline(yintercept = 0, colour = \u0026#34;lightgrey\u0026#34;, size=0.5) + geom_point( size = 0.5)+ geom_label_repel(size = 4, fill = \u0026#34;deepskyblue\u0026#34;, colour = \u0026#34;black\u0026#34;, min.segment.length = unit(0, \u0026#34;lines\u0026#34;)) #plot #dv.off() 20240229 IPA in R (文字篇) - FTTA (From Text to Action, From Mining to Meaning) 1.下載 noteapd++ https://notepad-plus-plus.org/downloads/ 2.中文情緒字典 TLSSD(Tsao, Lin, and Su SD) TLSSD.csv 3.中文停止詞 stopwordsutf8.csv 4.Jeiba_Tsao_SERVQ_IPA_0200_0312.R https://www.sidrlab.net/ 5.文字集 3J.xlsx 6.構面特徵關鍵字集: 3JDim.xlsx 7.參考文獻: Tsao ,Hsiu-Yuan, Colin Campbell, Sean Sands, Alexis Mavrommatis (2022),” From mining to meaning: How B2B marketers can leverage text to inform strategy”, Industrial Marketing Management , 106, pp.90-98. (SSCI, Impact Factor = 8.89, rank in Management (Q1))\n下載FTTA_R.ZIP 可得到2.到6.\nPython程式碼 import jieba import pandas as pd import nltk from nltk.tokenize import sent_tokenize from snownlp import SnowNLP import csv from wordcloud import WordCloud import matplotlib.pyplot as plt # 下載NLTK資源 nltk.download(\u0026#39;punkt\u0026#39;) # Function to load sentiment dictionary from file def load_sentiment_dictionary(filename): sentiment_dict = {} with open(filename, \u0026#39;r\u0026#39;, newline=\u0026#39;\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as csvfile: reader = csv.reader(csvfile) for row in reader: word, score = row sentiment_dict[word] = int(score) return sentiment_dict # Load sentiment dictionary from file sentiment_dict = load_sentiment_dictionary(\u0026#39;TLSSD.csv\u0026#39;) import re def split_chinese_sentences(text): # 定义中文标点符号的正则表达式模式 chinese_punctuation_pattern = r\u0026#39;[！？。；]\u0026#39; # 使用正则表达式模式分割文本 sentences = re.split(chinese_punctuation_pattern, text) # 移除空字符串 sentences = [sentence.strip() for sentence in sentences if sentence.strip()] return sentences # 拆解中文句子 #sentences = split_chinese_sentences(text) # 計算句子的情緒分數 def calculate_emotion_score(sentence, sentiment_dict): words = jieba.lcut(sentence) score = [] avg_sentence_scores = [] for word in words: if word in sentiment_dict: score.append(sentiment_dict[word]) if score: avg_sentiment_score = sum(score) / len(score) else: avg_sentiment_score = 0 avg_sentence_scores.append(avg_sentiment_score) return avg_sentence_scores # 定義函式來處理文件 def process_file(file_path): # 儲存行數、句子序號、句子和情緒分數的列表 line_numbers = [] sentence_numbers = [] sentences = [] sentiment_scores = [] # Initialize a list to store average sentence scores emotion_scores = [] # 開啟文件並逐行處理 with open(file_path, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as file: for line_number, line in enumerate(file, start=1): # 將每一行拆解成句子 #line_sentences = sent_tokenize(line.strip()) line_sentences = split_chinese_sentences(line) # 記錄每個句子的行數、句子序號、句子本身和情緒分數 for sentence_number, sentence in enumerate(line_sentences, start=1): line_numbers.append(line_number) sentence_numbers.append(sentence_number) sentences.append(sentence) # 使用SnowNLP進行情緒分析 #sentiment_score = SnowNLP(sentence).sentiments #sentiment_scores.append(sentiment_score) emotion_scores.append(calculate_emotion_score(sentence, sentiment_dict)) # 將行數、句子序號、句子和情緒分數記錄成DataFrame df = pd.DataFrame({\u0026#39;Line Number\u0026#39;: line_numbers, \u0026#39;Sentence Number\u0026#39;: sentence_numbers, \u0026#39;Sentence\u0026#39;: sentences, \u0026#39;Sentiment Score\u0026#39;: emotion_scores}) return df # 調用函式並讀取中文文字檔 #file_path = \u0026#39;shopify.txt\u0026#39; # 請將檔案路徑替換為你的文字檔路徑 file_path = \u0026#39;3J.txt\u0026#39; # 請將檔案路徑替換為你的文字檔路徑 text_dataframe = process_file(file_path) # 顯示DataFrame print(text_dataframe) #--------------------------------------------------------------------------- # 根據關鍵字判斷構面的函數 def classify_aspect(sentence, aspect_keywords): words = jieba.lcut(sentence) for word in words: for aspect, keywords in aspect_keywords.items(): if word in keywords: return aspect return \u0026#39;unknown\u0026#39; # 從文件中讀取不同構面特徵關鍵字 def load_aspect_keywords(file_path): aspect_keywords = {} with open(file_path, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as file: for line in file: aspect, *keywords = line.strip().split() aspect_keywords[aspect] = keywords return aspect_keywords # 測試句子 #test_sentences = [ # \u0026#34;這家餐廳的菜好吃，服務也很好。\u0026#34;, # \u0026#34;這本書的內容非常有趣。\u0026#34;, # \u0026#34;這個產品的品質不怎麼樣。\u0026#34;, # \u0026#34;今天的天氣真糟糕，讓人感到很沮喪。\u0026#34;, # \u0026#34;這個活動很一般，沒有特別的感覺。\u0026#34; #] test_sentences = text_dataframe # 文件路徑 keywords_file_path = \u0026#39;3JDims.txt\u0026#39; # 加載關鍵字 aspect_keywords = load_aspect_keywords(keywords_file_path) aspects = [] sentences = [] # 進行構面分類並標註的主程式 for sentence in text_dataframe[\u0026#39;Sentence\u0026#39;]: aspect = classify_aspect(sentence, aspect_keywords) sentences.append(sentence) aspects.append(aspect) print(f\u0026#34;句子: \u0026#39;{sentence}\u0026#39;，構面: {aspect}\u0026#34;) text_dataframe[\u0026#39;aspect\u0026#39;] = aspects df = text_dataframe #view data types for each column #df.dtypes sentiment_sores = [] for i in range(len(df)): sentiment_sores.append(df.iloc[i,3][0]) df[\u0026#39;Sentiment Score\u0026#39;] = sentiment_sores #------------------------------------------ positive_df = df[df[\u0026#39;Sentiment Score\u0026#39;] \u0026gt; 0] negative_df = df[df[\u0026#39;Sentiment Score\u0026#39;] \u0026lt; 0] #------------------------------------------ text=positive_df[\u0026#39;Sentence\u0026#39;].values.tolist() # 使用jieba進行分詞 text = \u0026#39; \u0026#39;.join(text) wordlist = jieba.cut(text, cut_all=False) positive_process_text = \u0026#39; \u0026#39;.join(wordlist) text=negative_df[\u0026#39;Sentence\u0026#39;].values.tolist() # 使用jieba進行分詞 text = \u0026#39; \u0026#39;.join(text) wordlist = jieba.cut(text, cut_all=False) negative_process_text = \u0026#39; \u0026#39;.join(wordlist) # 加载停用词列表 #------------------------------------------------------- # WordCloud #------------------------------------------------------- # 文本预处理函数，去除停用词 def generate_wordcloud(text): # 去除停用词后的文本 stopwords = set() with open(\u0026#39;stopwordsutf8.txt\u0026#39;, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: for line in f: stopwords.add(line.strip()) words = jieba.cut(text) filtered_words = [word for word in words if word not in stopwords] processed_text = \u0026#39; \u0026#39;.join(filtered_words) #wget https://raw.githubusercontent.com/victorgau/wordcloud/master/SourceHanSansTW-Regular.otf font_path=\u0026#39;SourceHanSansTW-Regular.otf\u0026#39; # 創建文字雲 wordcloud = WordCloud(font_path=\u0026#39;SourceHanSansTW-Regular.otf\u0026#39;, background_coloR\u0026#39;white\u0026#39;).generate(processed_text) # 使用matplotlib展示文字雲 plt.imshow(wordcloud, interpolation=\u0026#39;bilinear\u0026#39;) plt.axis(\u0026#34;off\u0026#34;) plt.show() pass # 去除停用词后的文本 #positive_processed_text = remove_stopwords(positive_wordlist_space_split) generate_wordcloud(positive_process_text) #negative_processed_text = remove_stopwords(negative_wordlist_space_split) generate_wordcloud(negative_process_text) #------------------------------------------------------- # IPA Matrix #------------------------------------------------------- def calculate_value_counts_and_mean(data_frame, count_column, value_column): \u0026#34;\u0026#34;\u0026#34; 计算DataFrame中某个列的值出现的次数以及相应的另一个列的平均值，并将结果转换为DataFrame :param data_frame: DataFrame对象 :param count_column: 用于计算值出现次数的列名 :param value_column: 用于计算平均值的列名 :return: 返回一个DataFrame，包含计数、平均值和Attribute列，索引为不同的值 \u0026#34;\u0026#34;\u0026#34; result = {} grouped = data_frame.groupby(count_column) for name, group in grouped: value_counts = group[value_column].count() mean_value = group[value_column].mean() result[name] = (value_counts, mean_value) result_df = pd.DataFrame.from_dict(result, orient=\u0026#39;index\u0026#39;, columns=[\u0026#39;Importance\u0026#39;, \u0026#39;Sentiment\u0026#39;]).reset_index() result_df.rename(columns={\u0026#39;index\u0026#39;: \u0026#39;Attribute\u0026#39;}, inplace=True) return result_df # 要计算的列名 count_column = \u0026#39;aspect\u0026#39; value_column = \u0026#39;Sentiment Score\u0026#39; # 调用函数计算列值出现次数和对应的平均值，并转换为DataFrame result_df = calculate_value_counts_and_mean(df, count_column, value_column) print(result_df) #------------------------------------------------------- # Plot IPA Matrix #------------------------------------------------------- # Plot df = result_df[result_df[\u0026#39;Attribute\u0026#39;] != \u0026#39;unknown\u0026#39;] fig, ax = plt.subplots() #df.plot(kind=\u0026#39;scatter\u0026#39;, x=\u0026#39;Importance\u0026#39;, y=\u0026#39;Performance\u0026#39;, ax=ax) df.plot(kind=\u0026#39;scatter\u0026#39;, x=\u0026#39;Sentiment\u0026#39;, y=\u0026#39;Importance\u0026#39;, ax=ax) # Label points in the scatter plot for i, txt in enumerate(df.Attribute): ax.annotate(txt, (df.Sentiment[i],df.Importance[i] )) # Set chart title and labels ax.set_title(\u0026#39;FTTA Analysis\u0026#39;) ax.set_xlabel(\u0026#39;Sentiment\u0026#39;) ax.set_ylabel(\u0026#39;Importance\u0026#39;) # Draw mean lines for importance and performance mean_importance = df[\u0026#39;Importance\u0026#39;].mean() mean_performance = df[\u0026#39;Sentiment\u0026#39;].mean() ax.axvline(x=mean_performance, coloR\u0026#39;r\u0026#39;, linestyle=\u0026#39;--\u0026#39;) ax.axhline(y=mean_importance, coloR\u0026#39;r\u0026#39;, linestyle=\u0026#39;--\u0026#39;) # Show plot plt.show() R語言程式碼 Sys.setlocale(category = \u0026#34;LC_ALL\u0026#34;, locale = \u0026#34;C.UTF-8\u0026#34;) library(data.table) library(jiebaR) library(wordcloud) library(stringr) library(tm) library(dplyr) library(tidyverse) library(tidytext) library(purrr) library(reshape2) library(wordcloud) library(RColorBrewer) library(tmcn) library(wordcloud2) library(webshot) library(htmlwidgets) library(igraph) library(ggraph) library(widyr) library(ggplot2) library(ggrepel) library(xlsx) #--------------- test code ------------- filename \u0026lt;- \u0026#39;3J.xlsx\u0026#39; #--------------- test code ------------- TripText = NULL #tenlong \u0026lt;- fread(file=filename,sep = \u0026#34;,\u0026#34; ,encoding=\u0026#34;UTF-8\u0026#34;,headeRFALSE, stringsAsFactors=FALSE) tenlong \u0026lt;- read.xlsx(filename, sheetIndex = 1) colnames(tenlong) \u0026lt;- c(\u0026#34;text\u0026#34;) tenlong \u0026lt;- tenlong %\u0026gt;% mutate(tenlong,line=c(1:nrow(tenlong))) %\u0026gt;% relocate(line) tenlong \u0026lt;- tenlong[which(nchar(tenlong$text) \u0026gt;2),] colnames(tenlong) \u0026lt;- c(\u0026#34;line\u0026#34;,\u0026#34;text\u0026#34;) #============================================================================= # 斷句 (稍後解釋) #============================================================================= AZDF \u0026lt;- tenlong test_text \u0026lt;- NULL test_text \u0026lt;- AZDF test_text$text \u0026lt;- gsub(pattern = \u0026#39;。\u0026#39;,replacement = \u0026#39;.\u0026#39;,test_text$text) test_text$text \u0026lt;- gsub(pattern = \u0026#39;，\u0026#39;,replacement = \u0026#39;,\u0026#39;,test_text$text) test_text$text \u0026lt;- gsub(pattern = \u0026#39;！\u0026#39;,replacement = \u0026#39;.\u0026#39;,test_text$text) test_text$text \u0026lt;- gsub(pattern = \u0026#39;；\u0026#39;,replacement = \u0026#39;.\u0026#39;,test_text$text) test_text$text \u0026lt;- gsub(pattern = \u0026#39;？\u0026#39;,replacement = \u0026#39;.\u0026#39;,test_text$text) test_text$text split_into_sentences \u0026lt;- function(text){ #This is a function built off this Python solution that allows some flexibility in that the lists of prefixes, suffixes, etc. can be modified to your specific text. It\u0026#39;s definitely not perfect, but could be useful with the right text. caps = \u0026#34;([A-Z])\u0026#34; prefixes = \u0026#34;(Mr|St|Mrs|Ms|Dr|Prof|Capt|Cpt|Lt|Mt)\\\\.\u0026#34; suffixes = \u0026#34;(Inc|Ltd|Jr|Sr|Co)\u0026#34; acronyms = \u0026#34;([A-Z][.][A-Z][.](?:[A-Z][.])?)\u0026#34; starters = \u0026#34;(Mr|Mrs|Ms|Dr|He\\\\s|She\\\\s|It\\\\s|They\\\\s|Their\\\\s|Our\\\\s|We\\\\s|But\\\\s|However\\\\s|That\\\\s|This\\\\s|Wherever)\u0026#34; websites = \u0026#34;\\\\.(com|edu|gov|io|me|net|org)\u0026#34; digits = \u0026#34;([0-9])\u0026#34; text = gsub(\u0026#34;\\n|\\r\\n\u0026#34;,\u0026#34; \u0026#34;, text) text = gsub(prefixes, \u0026#34;\\\\1\u0026lt;prd\u0026gt;\u0026#34;, text) text = gsub(websites, \u0026#34;\u0026lt;prd\u0026gt;\\\\1\u0026#34;, text) text = gsub(\u0026#39;www\\\\.\u0026#39;, \u0026#34;www\u0026lt;prd\u0026gt;\u0026#34;, text) text = gsub(\u0026#34;Ph.D.\u0026#34;,\u0026#34;Ph\u0026lt;prd\u0026gt;D\u0026lt;prd\u0026gt;\u0026#34;, text) text = gsub(paste0(\u0026#34;\\\\s\u0026#34;, caps, \u0026#34;\\\\. \u0026#34;), \u0026#34; \\\\1\u0026lt;prd\u0026gt; \u0026#34;, text) text = gsub(paste0(acronyms, \u0026#34; \u0026#34;, starters), \u0026#34;\\\\1\u0026lt;stop\u0026gt; \\\\2\u0026#34;, text) text = gsub(paste0(caps, \u0026#34;\\\\.\u0026#34;, caps, \u0026#34;\\\\.\u0026#34;, caps, \u0026#34;\\\\.\u0026#34;), \u0026#34;\\\\1\u0026lt;prd\u0026gt;\\\\2\u0026lt;prd\u0026gt;\\\\3\u0026lt;prd\u0026gt;\u0026#34;, text) text = gsub(paste0(caps, \u0026#34;\\\\.\u0026#34;, caps, \u0026#34;\\\\.\u0026#34;), \u0026#34;\\\\1\u0026lt;prd\u0026gt;\\\\2\u0026lt;prd\u0026gt;\u0026#34;, text) text = gsub(paste0(\u0026#34; \u0026#34;, suffixes, \u0026#34;\\\\. \u0026#34;, starters), \u0026#34; \\\\1\u0026lt;stop\u0026gt; \\\\2\u0026#34;, text) text = gsub(paste0(\u0026#34; \u0026#34;, suffixes, \u0026#34;\\\\.\u0026#34;), \u0026#34; \\\\1\u0026lt;prd\u0026gt;\u0026#34;, text) text = gsub(paste0(\u0026#34; \u0026#34;, caps, \u0026#34;\\\\.\u0026#34;), \u0026#34; \\\\1\u0026lt;prd\u0026gt;\u0026#34;,text) text = gsub(paste0(digits, \u0026#34;\\\\.\u0026#34;, digits), \u0026#34;\\\\1\u0026lt;prd\u0026gt;\\\\2\u0026#34;, text) text = gsub(\u0026#34;...\u0026#34;, \u0026#34;\u0026lt;prd\u0026gt;\u0026lt;prd\u0026gt;\u0026lt;prd\u0026gt;\u0026#34;, text, fixed = TRUE) text = gsub(\u0026#39;\\\\.”\u0026#39;, \u0026#39;”.\u0026#39;, text) text = gsub(\u0026#39;\\\\.\u0026#34;\u0026#39;, \u0026#39;\\\u0026#34;.\u0026#39;, text) text = gsub(\u0026#39;\\\\!\u0026#34;\u0026#39;, \u0026#39;\u0026#34;!\u0026#39;, text) text = gsub(\u0026#39;\\\\?\u0026#34;\u0026#39;, \u0026#39;\u0026#34;?\u0026#39;, text) text = gsub(\u0026#39;\\\\.\u0026#39;, \u0026#39;.\u0026lt;stop\u0026gt;\u0026#39;, text) text = gsub(\u0026#39;\\\\?\u0026#39;, \u0026#39;?\u0026lt;stop\u0026gt;\u0026#39;, text) text = gsub(\u0026#39;\\\\!\u0026#39;, \u0026#39;!\u0026lt;stop\u0026gt;\u0026#39;, text) text = gsub(\u0026#39;\u0026lt;prd\u0026gt;\u0026#39;, \u0026#39;.\u0026#39;, text) sentence = strsplit(text, \u0026#34;\u0026lt;stop\u0026gt;\\\\s*\u0026#34;) return(sentence) } df_sentences \u0026lt;- NULL for (i in 1: nrow(AZDF)) { df_sentences_t \u0026lt;- NULL sentences \u0026lt;- split_into_sentences(test_text$text[i]) names(sentences) \u0026lt;- \u0026#39;sentence\u0026#39; df_sentences_t \u0026lt;- dplyr::bind_rows(sentences) #chapterno \u0026lt;- rep(test_text$chapter[i],times=nrow(df_sentences_t)) lineno \u0026lt;- rep(test_text$line[i],times=nrow(df_sentences_t)) #dateno \u0026lt;- rep(test_text$t1date[i],times=nrow(df_sentences_t)) sentenceno \u0026lt;- c(1:nrow(df_sentences_t)) df_sentences_t \u0026lt;- dplyr::bind_cols(lineno, sentenceno,df_sentences_t) colnames(df_sentences_t) \u0026lt;- c(\u0026#34;line\u0026#34;,\u0026#34;sentenceno\u0026#34;,\u0026#34;text\u0026#34;) df_sentences \u0026lt;- dplyr::bind_rows(df_sentences,df_sentences_t) } AZDF01 \u0026lt;- df_sentences AZDF01$text=gsub(\u0026#34;\\\\.\u0026#34;,\u0026#34;\u0026#34;,AZDF01$text) AZDF02 \u0026lt;- AZDF01[which(nchar(AZDF01$text) \u0026gt;1),] TripText \u0026lt;- AZDF02[,-2] TripText01 \u0026lt;- TripText$line TripText02 \u0026lt;- TripText$text #============================================================================= # 中文情緒字典 TLSSD(Tsao \u0026amp; Lin \u0026amp; Su) 情緒字典 #============================================================================= afinn_list \u0026lt;- fread(file=\u0026#39;TLSSD.csv\u0026#39;,encoding=\u0026#34;UTF-8\u0026#34;,headeRFALSE, stringsAsFactors=FALSE) names(afinn_list) \u0026lt;- c(\u0026#39;word\u0026#39;, \u0026#39;score\u0026#39;) afinn_list$word \u0026lt;- tolower(afinn_list$word) #------------- get_noun ---------------------- get_noun = function(x){ stopifnot(inherits(x,\u0026#34;character\u0026#34;)) index = names(x) %in% c(\u0026#34;n\u0026#34;,\u0026#34;nr\u0026#34;,\u0026#34;nr1\u0026#34;,\u0026#34;nr2\u0026#34;,\u0026#34;nrj\u0026#34;,\u0026#34;nrf\u0026#34;,\u0026#34;ns\u0026#34;,\u0026#34;nsf\u0026#34;,\u0026#34;nt\u0026#34;,\u0026#34;nz\u0026#34;, \u0026#34;nl\u0026#34;,\u0026#34;ng\u0026#34;, \u0026#34;a\u0026#34;,\u0026#34;ad\u0026#34;,\u0026#34;an\u0026#34;,\u0026#34;ag\u0026#34;,\u0026#34;al\u0026#34;, \u0026#34;v\u0026#34;,\u0026#34;vh\u0026#34;,\u0026#34;vg\u0026#34;,\u0026#34;vd\u0026#34;,\u0026#34;vn\u0026#34;,\u0026#34;vi*\u0026#34;,\u0026#34;vq*\u0026#34;, \u0026#34;d\u0026#34;,\u0026#34;x\u0026#34;,\u0026#34;b\u0026#34;) x[index] } #---------------tokenizer-------------------- chi_tokenizerN \u0026lt;- function (t) { lapply(t, function(x){ tokens \u0026lt;- segment(x, jieba_tokenizerN) tokens \u0026lt;- get_noun(tokens[nchar(tokens) \u0026gt; 1]) #tokens \u0026lt;- tokens[nchar(tokens) \u0026gt; 1] return(tokens) }) } #---------------user define stop words ------ swords \u0026lt;- fread(file=\u0026#39;stopwordsutf8.csv\u0026#39;,encoding=\u0026#34;UTF-8\u0026#34;,headeRFALSE, stringsAsFactors=FALSE) stopwords \u0026lt;- data_frame(word=swords$V1) # --------------------Jieba TAG and User Define Dict ----- #jieba_tokenizer = worker(type=\u0026#34;tag\u0026#34;, user = \u0026#34;three_kingdoms_lexicon.traditional.dict\u0026#34;) #jieba_tokenizer = worker(user = \u0026#34;three_kingdoms_lexicon.traditional.dict\u0026#34;) #jieba_tokenizer = worker(\u0026#34;tag\u0026#34;,write = \u0026#34;NOFILE\u0026#34;) #jieba_tokenizer = worker(write = \u0026#34;NOFILE\u0026#34;,user = \u0026#34;tenlong8_user_dict.utf8\u0026#34;) #jieba_tokenizer = worker() #------------------------------------------------------------ #jieba_tokenizer = worker(\u0026#34;tag\u0026#34;,write = \u0026#34;NOFILE\u0026#34;) jieba_tokenizerN = worker(type=\u0026#34;tag\u0026#34;) #---------------- unnest ------------------------------- new_user_word(jieba_tokenizerN, c(\u0026#34;蔡英文\u0026#34;, \u0026#34;韓國瑜\u0026#34;,\u0026#34;國安五法\u0026#34;,\u0026#34;中共代理人法\u0026#34;, \u0026#34;李登輝\u0026#34;,\u0026#34;馬英九\u0026#34;,\u0026#34;蔣經國\u0026#34;,\u0026#34;陳水扁\u0026#34;,\u0026#34;柯文哲\u0026#34;, \u0026#34;王美花\u0026#34;,\u0026#34;雞排\u0026#34;,\u0026#34;郭董\u0026#34;,\u0026#34;郭台銘\u0026#34;,\u0026#34;缺電\u0026#34;,\u0026#34;設備\u0026#34;, \u0026#34;台電\u0026#34;,\u0026#34;限電\u0026#34;,\u0026#34;電力\u0026#34;,\u0026#34;三接\u0026#34;,\u0026#34;下台\u0026#34; )) tokens \u0026lt;- AZDF02 %\u0026gt;% unnest_tokens(word, text, token = chi_tokenizerN) %\u0026gt;% anti_join(stopwords) text_line \u0026lt;- tokens$line text_sentenceno \u0026lt;- tokens$sentenceno text_word \u0026lt;- tokens$word text_dataset \u0026lt;- data_frame(line=text_line,sentenceno=tokens$sentenceno,word=text_word ) #------------------ sentiment Analysis ------------------------------- #------------------calaulate score of sentiment --------------------- afinn_score_df \u0026lt;- text_dataset %\u0026gt;% inner_join(afinn_list,by=\u0026#34;word\u0026#34;) %\u0026gt;% group_by(line) %\u0026gt;% arrange(line) afinn_score_df sentiment_score_df_sentence \u0026lt;- afinn_score_df %\u0026gt;% group_by(line,sentenceno) %\u0026gt;% arrange(line) %\u0026gt;% summarise(sentiment=sum(as.numeric(score)) ) sentiment_score_df_sentence sentiment_score_df_sentence %\u0026gt;% summarise(sscore =mean(sentiment)) sentiment_score_df_sentence %\u0026gt;% ungroup() %\u0026gt;% summarise(sscore =mean(sentiment)) #------------------ Aspect / Dimensional ?i??------------------------- positive_lines_df \u0026lt;- sentiment_score_df_sentence %\u0026gt;% filter(sentiment \u0026gt; 0 ) %\u0026gt;% left_join(text_dataset, by=c(\u0026#34;line\u0026#34;,\u0026#34;sentenceno\u0026#34;)) positive_lines_df negative_lines_df \u0026lt;- sentiment_score_df_sentence %\u0026gt;% filter(sentiment \u0026lt; 0) %\u0026gt;% left_join(text_dataset,by=c(\u0026#34;line\u0026#34;,\u0026#34;sentenceno\u0026#34;)) negative_lines_df sentiment_words_df \u0026lt;- rbind(positive_lines_df,negative_lines_df) sentiment_words_df #------------------ restore sentences with score of sentiment ------ positive_lines_df_sentences \u0026lt;- unique(positive_lines_df[,1:3]) %\u0026gt;% left_join(AZDF02,by=c(\u0026#34;line\u0026#34;,\u0026#34;sentenceno\u0026#34;)) %\u0026gt;% left_join(sentiment_score_df_sentence,by=c(\u0026#34;line\u0026#34;,\u0026#34;sentenceno\u0026#34;)) positive_lines_df_sentences negative_lines_df_sentences \u0026lt;- unique(negative_lines_df[,1:3]) %\u0026gt;% left_join(AZDF02,by=c(\u0026#34;line\u0026#34;,\u0026#34;sentenceno\u0026#34;)) %\u0026gt;% left_join(sentiment_score_df_sentence,by=c(\u0026#34;line\u0026#34;,\u0026#34;sentenceno\u0026#34;)) negative_lines_df_sentences sentiment_line_df_sentences \u0026lt;- rbind(positive_lines_df_sentences,negative_lines_df_sentences) #-------------------------------------------------------- #---------------- frequency/counts of words------------ positive_tokens_count \u0026lt;- positive_lines_df %\u0026gt;% anti_join(stopwords) %\u0026gt;% #filter(chapter == cpt) %\u0026gt;% group_by(word) %\u0026gt;% summarise(n = n()) %\u0026gt;% filter(n \u0026gt; 1) %\u0026gt;% arrange(desc(n)) wordcloud2(positive_tokens_count) negative_tokens_count \u0026lt;- negative_lines_df %\u0026gt;% anti_join(stopwords) %\u0026gt;% #filter(chapter == cpt) %\u0026gt;% group_by(word) %\u0026gt;% summarise(sum = n()) %\u0026gt;% filter(sum \u0026gt; 1) %\u0026gt;% arrange(desc(sum)) wordcloud2(negative_tokens_count) #----------- 讀取構面特徵關鍵字--------------------------- #setwd(\u0026#34;c:/SIDRWEB/DIMS/\u0026#34;) #dimfilename \u0026lt;- \u0026#34;dimfile.csv\u0026#34; #diminfo \u0026lt;- fread(file=dimfilename,sep = \u0026#34;,\u0026#34; ,encoding=\u0026#34;UTF-8\u0026#34;,headeRFALSE, stringsAsFactors=FALSE) #dimensions \u0026lt;- diminfo$V1 #------------------Local Testing------------------------- selfdim_file_name \u0026lt;-\u0026#39;3JDim.xlsx\u0026#39; #------------------Local Testing------------------------- library(readxl) selfdimfile \u0026lt;- read_excel(selfdim_file_name) for(dim_name in unique(selfdimfile$Dimension)){ x \u0026lt;- subset( selfdimfile[which(selfdimfile$Dimension == dim_name) , 2 ]) #dimtxtname = paste0( selfdim_path,\u0026#34;/\u0026#34;,dim_name, \u0026#34;.txt\u0026#34;) dimtxtname = dim_name write.table(x, file = dimtxtname, row.names = FALSE, col.names = FALSE, quote = FALSE, sep = \u0026#34;\\t\u0026#34;)} dimensions \u0026lt;- unique(selfdimfile$Dimension) #------------------ Aspect / Dimensional ?i??------------------------- #setwd(\u0026#34;c:/Koong/csv/DataScience/SERVQ\u0026#34;) sentiment_words_df_words \u0026lt;- sentiment_score_df_sentence %\u0026gt;% left_join(text_dataset,by= c(\u0026#34;line\u0026#34;=\u0026#34;line\u0026#34;, \u0026#34;sentenceno\u0026#34;=\u0026#34;sentenceno\u0026#34;)) sentiment_words_df_words score_of_dimensions \u0026lt;- NULL score_of_dimensions \u0026lt;- lapply(dimensions, FUN= function(x){ dimname \u0026lt;- x dimname x \u0026lt;- fread(file=x,encoding=\u0026#34;UTF-8\u0026#34;,headeRFALSE, stringsAsFactors=FALSE) colnames(x) \u0026lt;- c(\u0026#34;word\u0026#34;) anz_words \u0026lt;- tolower(x$word) sentiment_DF \u0026lt;- sentiment_words_df_words %\u0026gt;% filter(word %in% anz_words) %\u0026gt;% #------------?O?_?????葖??A?p??--------- filter(as.numeric(sentiment) != 0) %\u0026gt;% #------------------------------------------- mutate(dimname = dimname) %\u0026gt;% arrange(line) #%\u0026gt;% # summarise(sentiment=mean(as.numeric(sentiment))) # return(summarise(sentiment_DF, sscore =mean(sentiment))) } ) sdf \u0026lt;- NULL for (i in 1:length(score_of_dimensions)) { sdf \u0026lt;- rbind(as.data.frame(sdf),as.data.frame(score_of_dimensions[[i]])) } feature_words_df \u0026lt;- sdf %\u0026gt;% left_join(sentiment_line_df_sentences,by=c(\u0026#34;line\u0026#34;,\u0026#34;sentenceno\u0026#34;)) %\u0026gt;% arrange(line) feature_words_df \u0026lt;- feature_words_df[,-8] feature_words_df \u0026lt;- feature_words_df[,-6] #-------------------------------------------------------- score_of_dim \u0026lt;- feature_words_df %\u0026gt;% filter(as.numeric(sentiment) != 0) %\u0026gt;% group_by(dimname) %\u0026gt;% summarise(sentiment=mean(as.numeric(sentiment))) score_of_dim #-----------Bar Plot ------------------------------------------------------------- sent.score \u0026lt;- score_of_dim$sentiment names(sent.score) \u0026lt;- score_of_dim$dimname colorBlind.8 \u0026lt;- c( orange=\u0026#34;#E69F00\u0026#34;, skyblue=\u0026#34;#56B4E9\u0026#34;, bluegreen=\u0026#34;#009E73\u0026#34;, yellow=\u0026#34;#F0E442\u0026#34;, blue=\u0026#34;#0072B2\u0026#34;, reddish=\u0026#34;#D55E00\u0026#34;, purplish=\u0026#34;#CC79A7\u0026#34;,black=\u0026#34;#000099\u0026#34;,green=\u0026#34;#38D57D\u0026#34;, green01=\u0026#34;#B9CC57\u0026#34;,yellow01=\u0026#34;#FFFF00\u0026#34;, PINK=\u0026#34;#FF9999\u0026#34;) cols \u0026lt;- colorBlind.8[1:length(sent.score)] barplot(sent.score, xlab=\u0026#34;Dimensions\u0026#34;, ylab=\u0026#34;score of sentiment\u0026#34;, col = cols, cex.axis=0.6, cex.names = 0.6) #-----------IPA OLD OK----------------------------------------------------------------- ipa1 \u0026lt;- NULL ipa1 \u0026lt;- feature_words_df %\u0026gt;% group_by(dimname) %\u0026gt;% summarise(count = n()) %\u0026gt;% left_join(score_of_dim, by=c(\u0026#34;dimname\u0026#34;)) %\u0026gt;% mutate( cmove = count - mean(count) ) %\u0026gt;% mutate( smove = sentiment - mean(sentiment )) %\u0026gt;% data.frame() #---------------------------------------------------------- #-------------------------------------------------------------------- ipa \u0026lt;- ipa1 #--------IPA1 GGplot OK--------------------------------------------------------- empty_theme \u0026lt;- theme( plot.background = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.border = element_blank(), panel.background = element_blank(), axis.line = element_blank(), axis.ticks = element_blank(), axis.text.y = element_text(angle = 90)) #setwd(\u0026#34;/home/jodytsao/public_html/SDTACD\u0026#34;) #IPAfilename \u0026lt;- paste(currenttime,\u0026#34;IPA.png\u0026#34;, sep=\u0026#39;\u0026#39;) #png(filename=IPAfilename) #---------------------------------------------------------- #ggplot(\u0026lt;資料\u0026gt;)+ # 設置畫布 # geom_\u0026lt;幾何圖示類型\u0026gt;( # 畫上幾何圖示 # aes(\u0026lt;座標對應\u0026gt;) #座標對應 # ) #----------------------------------------------------------- ggplot(ipa, aes(x = smove, y = cmove, label = dimname)) + #ggplot(ipa, aes(y = cmove, x = smove, label = word)) + empty_theme + theme(panel.border = element_rect(colour = \u0026#34;lightgrey\u0026#34;, fill=NA, size=1))+ labs(title = \u0026#34;IPA analysis\u0026#34;, y = \u0026#34;Counts of Dimension Sentences\u0026#34;, x = \u0026#34;Sentiment Score of Dimension\u0026#34;) + geom_vline(xintercept = 0, colour = \u0026#34;lightgrey\u0026#34;, size=0.5) + geom_hline(yintercept = 0, colour = \u0026#34;lightgrey\u0026#34;, size=0.5) + geom_point( size = 0.5)+ geom_label_repel(size = 4, fill = \u0026#34;deepskyblue\u0026#34;, colour = \u0026#34;black\u0026#34;, min.segment.length = unit(0, \u0026#34;lines\u0026#34;)) plot #dev.off() #----------------------------------------- #----------- all words ------------------ #-------------------------------------------------------------------------------- ipa3 \u0026lt;- NULL ipa3 \u0026lt;- sentiment_words_df %\u0026gt;% group_by(word) %\u0026gt;% summarise(sentiment = mean(sentiment),count = n()) %\u0026gt;% filter( count \u0026gt; 20) %\u0026gt;% mutate( cmove = count - mean(count) ) %\u0026gt;% mutate( smove = sentiment - mean(sentiment )) %\u0026gt;% data.frame() #-------------------------------------------------------------------- ipa \u0026lt;- ipa3 #---------------------------------------------------------------------- empty_theme \u0026lt;- theme( plot.background = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.border = element_blank(), panel.background = element_blank(), axis.line = element_blank(), axis.ticks = element_blank(), axis.text.y = element_text(angle = 90)) ggplot(ipa, aes(y = cmove, x = smove, label = word)) + empty_theme + theme(panel.border = element_rect(colour = \u0026#34;lightgrey\u0026#34;, fill=NA, size=1))+ labs(title = \u0026#34;IPA analysis\u0026#34; , x = \u0026#34;Sentiment Score of Word\u0026#34;, y = \u0026#34;Counts of Word\u0026#34;) + geom_vline(xintercept = 0, colour = \u0026#34;lightgrey\u0026#34;, size=0.5) + geom_hline(yintercept = 0, colour = \u0026#34;lightgrey\u0026#34;, size=0.5) + geom_text_repel(max.overlaps = Inf)+ #always show all labels even overlap geom_point( size = 0.5) 20240416 1.策略調色盤之策略選擇分析影片 https://youtube.com/playlist?list=PLYRFygSk3wHB_MFivSr71ug6dO_6uL7IH\u0026si=LEH6YQOO4kx6FMNy 2.策略調色盤之策略選擇分析練習(小組作業)\n3.觀看過\u0026quot;1.策略調色盤之策略選擇分析影片\u0026quot;後，試著填答 \u0026ldquo;市場策略分析策略調色盤_練習.docx\u0026quot;這三個問題 分別以三個集團(ZARA，Tesla, 以及Coca Cola) 為例，根據其旗下的品牌，分別:回答下列問題: 1.請選擇最符合你當前策略的描述 從 ABCDE中選擇一個您的看法 2.請選擇最符合你認為的商業環境的描述。從 FGHIJ中選擇一個您的看法 3.你想要採用的策略是什麼?從 KLMNO選擇一個您的看法 上傳小組討論的看法: ex: ZARA: A,F,K Telsa: B,G,L Coca Cola: C,H,M 針對: Netflix :串流平台 自製影片 為例來回答\n20240424 對應分析 (1) A Simple Explanation of how Correspondence Analysis Works (2) XLSTAT Python程式碼(請自行確認是否能運作) # 匯入所需的函式庫 import pandas as pd from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA import matplotlib.pyplot as plt import seaborn as sns from scipy.stats import chi2_contingency # 設定工作目錄 import os os.chdir(\u0026#34;D:/Koong/csv/\u0026#34;) # 讀取 csv 檔案 mobile = pd.read_csv(\u0026#34;mobile.csv\u0026#34;, index_col=0) # 檢查資料集的前幾列 print(mobile.head()) # 進行卡方檢定 chi2, p, dof, ex = chi2_contingency(mobile) print(f\u0026#34;Chi-square: {chi2}, p-value: {p}\u0026#34;) # 執行對應分析 from prince import CA ca = CA(n_components=2) ca = ca.fit(mobile) # 獲取特徵值/方差 eig_val = ca.eigenvalues_ print(eig_val) # 畫 Scree Plot sns.set(style=\u0026#34;whitegrid\u0026#34;) plt.figure(figsize=(10, 6)) plt.bar(range(1, len(eig_val) + 1), eig_val, alpha=0.5, align=\u0026#39;center\u0026#39;, label=\u0026#39;individual explained variance\u0026#39;) plt.step(range(1, len(eig_val) + 1), eig_val.cumsum(), where=\u0026#39;mid\u0026#39;, label=\u0026#39;cumulative explained variance\u0026#39;) plt.ylabel(\u0026#39;Explained variance ratio\u0026#39;) plt.xlabel(\u0026#39;Principal components\u0026#39;) plt.title(\u0026#39;Scree Plot\u0026#39;) plt.axhline(y=33.33, coloR\u0026#39;r\u0026#39;, linestyle=\u0026#39;--\u0026#39;) plt.legend(loc=\u0026#39;best\u0026#39;) plt.show() # 畫 Biplot fig, ax = plt.subplots(figsize=(10, 8)) # 取得行與列的座標 row_coords = ca.row_coordinates(mobile) col_coords = ca.column_coordinates(mobile) # 繪製行的點 ax.scatter(row_coords[0], row_coords[1], alpha=0.5) for i, txt in enumerate(row_coords.index): ax.annotate(txt, (row_coords[0][i], row_coords[1][i]), alpha=0.75) # 繪製列的點 ax.scatter(col_coords[0], col_coords[1], alpha=0.5, coloR\u0026#39;red\u0026#39;) for i, txt in enumerate(col_coords.index): ax.annotate(txt, (col_coords[0][i], col_coords[1][i]), alpha=0.75, coloR\u0026#39;red\u0026#39;) ax.set_xlabel(\u0026#39;Dimension 1\u0026#39;) ax.set_ylabel(\u0026#39;Dimension 2\u0026#39;) ax.set_title(\u0026#39;Correspondence Analysis Biplot\u0026#39;) plt.grid() plt.show() # 繪製行的貢獻圖 row_contrib = ca.row_contributions(mobile) top_contrib1 = row_contrib[0].nlargest(10) top_contrib2 = row_contrib[1].nlargest(10) plt.figure(figsize=(10, 6)) sns.barplot(x=top_contrib1.values, y=top_contrib1.index, palette=\u0026#34;viridis\u0026#34;) plt.title(\u0026#39;Top 10 Contributions of Rows to Dimension 1\u0026#39;) plt.xlabel(\u0026#39;Contribution\u0026#39;) plt.ylabel(\u0026#39;Rows\u0026#39;) plt.show() plt.figure(figsize=(10, 6)) sns.barplot(x=top_contrib2.values, y=top_contrib2.index, palette=\u0026#34;viridis\u0026#34;) plt.title(\u0026#39;Top 10 Contributions of Rows to Dimension 2\u0026#39;) plt.xlabel(\u0026#39;Contribution\u0026#39;) plt.ylabel(\u0026#39;Rows\u0026#39;) plt.show() # 繪製行的顏色根據貢獻度 plt.figure(figsize=(10, 8)) sns.scatterplot(x=row_coords[0], y=row_coords[1], hue=row_contrib[0] + row_contrib[1], palette=\u0026#34;viridis\u0026#34;, legend=False) for i, txt in enumerate(row_coords.index): plt.annotate(txt, (row_coords[0][i], row_coords[1][i]), alpha=0.75) plt.xlabel(\u0026#39;Dimension 1\u0026#39;) plt.ylabel(\u0026#39;Dimension 2\u0026#39;) plt.title(\u0026#39;Correspondence Analysis Biplot (Rows Colored by Contribution)\u0026#39;) plt.grid() plt.show() R 語言程式碼 library(\u0026#34;FactoMineR\u0026#34;) library(\u0026#34;factoextra\u0026#34;) setwd(\u0026#34;D:/Koong/csv/\u0026#34;) mobile \u0026lt;- read.csv(\u0026#34;mobile.csv\u0026#34;, stringsAsFactors = TRUE) colnames(mobile) \u0026lt;- c(NA,\u0026#34;Screen\u0026#34;,\u0026#34;Price\u0026#34;,\u0026#34;Design\u0026#34;,\u0026#34;Battery\u0026#34;,\u0026#34;Software\u0026#34;,\u0026#34;Camera\u0026#34;) he ad(mobile) rownames(mobile) \u0026lt;- mobile[,1] mobile \u0026lt;- mobile[,-1] #the association is highly significant (chi-square: 1944.456, p = 0). chisq \u0026lt;- chisq.test(mobile) chisq #---------- compute CA ------------------ res.ca \u0026lt;- CA(mobile, graph = FALSE) print(res.ca) #Extract the eigenvalues/variances retained by each dimension (axis) eig.val \u0026lt;- get_eigenvalue(res.ca) eig.val fviz_screeplot(res.ca, addlabels = TRUE, ylim = c(0, 50)) #------------ Biplot ------------------------------------- fviz_screeplot(res.ca) + geom_hline(yintercept=33.33, linetype=2, coloR\u0026#34;red\u0026#34;) # repel= TRUE to avoid text overlapping (slow if many point) fviz_ca_biplot(res.ca, repel = TRUE) #---------------------------------------------- fviz_ca_row(res.ca, col.row = \u0026#34;cos2\u0026#34;, gradient.cols = c(\u0026#34;#00AFBB\u0026#34;, \u0026#34;#E7B800\u0026#34;, \u0026#34;#FC4E07\u0026#34;), repel = TRUE) # Contributions of rows to dimension 1 fviz_contrib(res.ca, choice = \u0026#34;row\u0026#34;, axes = 1, top = 10) # Contributions of rows to dimension 2 fviz_contrib(res.ca, choice = \u0026#34;row\u0026#34;, axes = 2, top = 10) #-------------------------------------------------------- fviz_ca_row(res.ca, col.row = \u0026#34;contrib\u0026#34;, gradient.cols = c(\u0026#34;#00AFBB\u0026#34;, \u0026#34;#E7B800\u0026#34;, \u0026#34;#FC4E07\u0026#34;), repel = TRUE) 20240430 Clustering Analysis / Segmentation\n1.人口統計: USCity.xlsx / ClusterMean0501.R -\u0026gt; USCities.cvs\n2.RFM分析: onlinestore1220.xlsx /ClusterSegment_0501.R onlinestore1220.csv\nCluster_K-Means R語言程式碼 library(cluster) library(factoextra) # setwd(\u0026#34;/home/jodytsao/DataScienceMarketing/csv/\u0026#34;) USCity \u0026lt;- read.csv(\u0026#34;USCities.csv\u0026#34;) res.data01 \u0026lt;- USCity[3:8] USCityData \u0026lt;- as.data.frame(scale(res.data01)) #-------------------------------------------------------------------------------- for(n_cluster in 2:8){ cluster \u0026lt;- kmeans(USCityData, n_cluster) silhouetteScore \u0026lt;- mean( silhouette( cluster$cluster, dist(USCityData, method = \u0026#34;euclidean\u0026#34;) )[,3] ) print(sprintf(\u0026#39;Silhouette Score for %i Clusters: %0.4f\u0026#39;, n_cluster, silhouetteScore)) } #--------------------------------------------------------------------------------- # for reproducibility set.seed(123) # Visualize USCityDataClusterData \u0026lt;- kmeans(USCityData, 4) #### 3. Customer Segmentation via K-Means Clustering #### # cluster centers USCityDataClusterData$centers # cluster labels USCityCluster \u0026lt;- USCity %\u0026gt;% mutate(ClusteRUSCityDataClusterData$cluster) USCityCluster %\u0026gt;% group_by(Cluster) %\u0026gt;% summarise(Count=n()) # High value cluster summary summary(USCityCluster [which(USCityCluster $Cluster == 4),]) USCityCluster[which(USCityCluster$CityNo %in% which(USCityCluster$Cluster == 3)),] %\u0026gt;% group_by(City) USCityCluster[which(USCityCluster$CityNo %in% which(USCityCluster$Cluster == 1)),] %\u0026gt;% group_by(City) USCityCluster[which(USCityCluster$CityNo %in% which(USCityCluster$Cluster == 2)),] %\u0026gt;% group_by(City) USCityCluster[which(USCityCluster$CityNo %in% which(USCityCluster$Cluster == 4)),] %\u0026gt;% group_by(City) Customer Segmentation R語言程式碼 library(dplyr) library(ggplot2) library(cluster) #### 1. Load Data #### setwd(\u0026#34;/home/jodytsao/DataScienceMarketing/\u0026#34;) df1=read.csv(\u0026#34;csv/OnlineStore1220.csv\u0026#34;,headeRTRUE,stringsAsFactors = TRUE) res.data01 \u0026lt;- df1[3:5] normalizedDF \u0026lt;- as.data.frame(scale(res.data01)) #### 3. Customer Segmentation via K-Means Clustering #### cluster \u0026lt;- kmeans(normalizedDF, 4) # cluster centers cluster$centers # cluster labels # cluster labels df2 \u0026lt;- df1 %\u0026gt;% mutate(ClusteRcluster$cluster) #df2 %\u0026gt;% group_by(Cluster) %\u0026gt;% summarise(Count=n()) df2[df2$Cluster == 1,] %\u0026gt;% group_by(CustomerID) #--------------------------------------- # Selecting the best number of cluster #--------------------------------------- for(n_cluster in 2:8){ cluster \u0026lt;- kmeans(normalizedDF[c(\u0026#34;TotalSales\u0026#34;, \u0026#34;OrderCount\u0026#34;, \u0026#34;AvgOrderValue\u0026#34;)], n_cluster) silhouetteScore \u0026lt;- mean( silhouette( cluster$cluster, dist(normalizedDF[c(\u0026#34;TotalSales\u0026#34;, \u0026#34;OrderCount\u0026#34;, \u0026#34;AvgOrderValue\u0026#34;)], method = \u0026#34;euclidean\u0026#34;) )[,3] ) print(sprintf(\u0026#39;Silhouette Score for %i Clusters: %0.4f\u0026#39;, n_cluster, silhouetteScore)) } #--------------------------------------- # Selecting the best number of cluster #--------------------------------------- Cluster_K-Means Python程式碼(請自行確認是否能運作) # 匯入所需的函式庫 import pandas as pd from sklearn.preprocessing import StandardScaler from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score import numpy as np # 設定工作目錄 import os os.chdir(\u0026#34;/home/jodytsao/DataScienceMarketing/csv/\u0026#34;) # 讀取 csv 檔案 USCity = pd.read_csv(\u0026#34;USCities.csv\u0026#34;) # 擷取所需的資料欄位並進行標準化 res_data01 = USCity.iloc[:, 2:8] scaler = StandardScaler() USCityData = scaler.fit_transform(res_data01) # 計算不同群數的輪廓係數 (Silhouette Score) for n_cluster in range(2, 9): kmeans = KMeans(n_clusters=n_cluster, random_state=0) cluster_labels = kmeans.fit_predict(USCityData) silhouette_avg = silhouette_score(USCityData, cluster_labels) print(f\u0026#39;Silhouette Score for {n_cluster} Clusters: {silhouette_avg:.4f}\u0026#39;) # 設置隨機種子以確保結果可重現 np.random.seed(123) # 進行 K-means 聚類，使用 4 個群 kmeans = KMeans(n_clusters=4, random_state=0) USCityDataClusterData = kmeans.fit(USCityData) # 聚類中心 print(USCityDataClusterData.cluster_centers_) # 加入群集標籤到原始資料中 USCity[\u0026#39;Cluster\u0026#39;] = USCityDataClusterData.labels_ # 每個群集的數量統計 print(USCity.groupby(\u0026#39;Cluster\u0026#39;).size()) # 高價值群集的摘要統計 print(USCity[USCity[\u0026#39;Cluster\u0026#39;] == 3].describe()) # 各群集內的城市名稱統計 for cluster_num in range(4): cities_in_cluster = USCity[USCity[\u0026#39;Cluster\u0026#39;] == cluster_num][\u0026#39;City\u0026#39;] print(f\u0026#39;Cities in Cluster {cluster_num}:\u0026#39;) print(cities_in_cluster) Customer Segmentation Python程式碼(請自行確認是否能運作) # 匯入所需的函式庫 import pandas as pd from sklearn.preprocessing import StandardScaler from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score import numpy as np # 設定工作目錄 import os os.chdir(\u0026#34;/home/jodytsao/DataScienceMarketing/\u0026#34;) # 讀取 csv 檔案 df1 = pd.read_csv(\u0026#34;csv/OnlineStore1220.csv\u0026#34;) # 擷取所需的資料欄位並進行標準化 res_data01 = df1.iloc[:, 2:5] scaler = StandardScaler() normalizedDF = pd.DataFrame(scaler.fit_transform(res_data01), columns=res_data01.columns) # 進行 K-means 聚類，使用 4 個群 kmeans = KMeans(n_clusters=4, random_state=0) kmeans.fit(normalizedDF) # 聚類中心 print(kmeans.cluster_centers_) # 加入群集標籤到原始資料中 df1[\u0026#39;Cluster\u0026#39;] = kmeans.labels_ # 每個群集的數量統計 print(df1.groupby(\u0026#39;Cluster\u0026#39;).size()) # 群集 1 中的客戶統計 print(df1[df1[\u0026#39;Cluster\u0026#39;] == 1].groupby(\u0026#39;CustomerID\u0026#39;).size()) # 計算不同群數的輪廓係數 (Silhouette Score) for n_cluster in range(2, 9): kmeans = KMeans(n_clusters=n_cluster, random_state=0) cluster_labels = kmeans.fit_predict(normalizedDF[[\u0026#39;TotalSales\u0026#39;, \u0026#39;OrderCount\u0026#39;, \u0026#39;AvgOrderValue\u0026#39;]]) silhouette_avg = silhouette_score(normalizedDF[[\u0026#39;TotalSales\u0026#39;, \u0026#39;OrderCount\u0026#39;, \u0026#39;AvgOrderValue\u0026#39;]], cluster_labels) print(f\u0026#39;Silhouette Score for {n_cluster} Clusters: {silhouette_avg:.4f}\u0026#39;) 20240508 Association Rule (關聯法則 啤酒與尿布) 指令一：\n請你記住以下四個構面和定義，以協助後面歸納，分別是：\nExperience：遊客在旅行中所感受到的一切，這個詞能夠綜合表達旅行帶來的各種感受和體驗，以及對這些體驗的總體評價。 Fulfillment：遊客對旅行的意義和滿足感，反映他們是否認為旅行有意義，以及是否從中獲得了個人滿足感。 Discovery：在旅行中遊客所獲得的新發現和學習，包括對自己的認識、新獲取的信息以及對新體驗的興奮感。 Unique：旅程中的獨特性，表明旅行是一次千載難逢的體驗，或者是一次能夠近距離體驗當地文化的機會。 再來，我有四個規則要請你遵守：\n歸納的時候，請幫我依據詞意的「相近性」進行分類 2.一個字只會被歸類到一個類別中。 3.只能出現我有給你的字，請勿加入其他在網路上搜到的字。 請根據上述規則，把下列的字詞進行歸納\n指令二（接續指令一的結果）：\n構面Experience的定義為：遊客在旅行中所感受到的一切，這個詞能夠綜合表達旅行帶來的各種感受和體驗，以及對這些體驗的總體評價。 請從下列187個單字中（指令一歸納在Experience的字詞），擷取出20個最適合用來衡量這個構面的單詞：\n構面Fulfillment的定義為：遊客對旅行的意義和滿足感，反映他們是否認為旅行有意義，以及是否從中獲得了個人滿足感。 請從下列22個單字中（指令一歸納在Fulfillment的字詞），萃取出20個最適合用來衡量這個構面的單詞：\n構面Discovery的定義為：在旅行中遊客所獲得的新發現和學習，包括對自己的認識、新獲取的信息以及對新體驗的興奮感。 請從下列103個單字中（指令一歸納在Discovery的字詞），萃取出20個最適合用來衡量這個構面的單詞：\n構面Unique的定義為：旅程中的獨特性，表明旅行是一次千載難逢的體驗，或者是一次能夠近距離體驗當地文化的機會。 請從下列164個單字中（指令一歸納在Unique的字詞），萃取出20個最適合用來衡量這個構面的單詞：\nRaddit Traveling to Taiwan Posts\nPrompt 1.\nAssumed you are a academic researhcer, based on the definition of Experience, Fulfillment, Discovery, and Uniquie as the following:\n(1) Experience It refers to travelers\u0026rsquo; mindset and experiences of seeking joy and happiness during their journey. This dimension highlights travelers\u0026rsquo; emphasis on the diversity of experiences and exploration while pursuing pleasure and enjoyment, as well as the positive emotions and satisfaction gained during the journey.\u0026rdquo;\n(2)Fulfillment It refers to the degree to which travelers find profound meaning in the overall journey, reflecting a desire to seek spiritual fulfillment and personal growth during the trip. This dimension encompasses travelers\u0026rsquo; pursuit of pleasure and novelty while also focusing on the deeper significance of travel, such as respect for culture, engagement in meaningful activities, and the pursuit of personal development.\n(3)Discovery It refers to the various knowledge and information gained by travelers during the journey. This dimension highlights the importance travelers place on seeking knowledge and learning during their travels, viewing travel as a pathway to enriching life experiences.\n(4)Unique It refers to the sense of novelty that travelers seek, experience, and cherish during their journey. This dimension emphasizes travelers\u0026rsquo; pursuit of diverse, unique, and captivating experiences, as well as the pleasure and fulfillment derived from these experiences.\nCould you categorize the following sentences, I will give you later, from consumer reviews based on the definition of dimensions above of Experience, Fulfillment, Discovery, and Unique.\n2.Assumed you are a academic researcher, based on the definition of Experience, Fulfillment, Discovery, and Uniquie as the following:\n(1) Experience It refers to travelers\u0026rsquo; mindset and experiences of seeking joy and happiness during their journey. This dimension highlights travelers\u0026rsquo; emphasis on the diversity of experiences and exploration while pursuing pleasure and enjoyment, as well as the positive emotions and satisfaction gained during the journey.\u0026quot;\n(2)Fulfillment It refers to the degree to which travelers find profound meaning in the overall journey, reflecting a desire to seek spiritual fulfillment and personal growth during the trip. This dimension encompasses travelers\u0026rsquo; pursuit of pleasure and novelty while also focusing on the deeper significance of travel, such as respect for culture, engagement in meaningful activities, and the pursuit of personal development.\n(3)Discovery It refers to the various knowledge and information gained by travelers during the journey. This dimension highlights the importance travelers place on seeking knowledge and learning during their travels, viewing travel as a pathway to enriching life experiences.\n(4)Unique It refers to the sense of novelty that travelers seek, experience, and cherish during their journey. This dimension emphasizes travelers\u0026rsquo; pursuit of diverse, unique, and captivating experiences, as well as the pleasure and fulfillment derived from these experiences.\nCould you extract keywords from the following sentences of consumer reviews , I will give you later, based on the definition of dimensions above of Experience, Fulfillment, Discovery, and Unique.\n關聯法則 R語言程式碼 # 關聯法則 R語言程式碼 # Installing Packages # Loading package #Sys.setenv(JAVA_HOME=\u0026#39;C:\\\\Program Files\\\\Java\\\\jre1.8.0_291\u0026#39;) library(arules) library(arulesViz) library(xlsx) setwd(\u0026#34;c:/Koong/csv/DataScience\u0026#34;) beeRread.xlsx(\u0026#34;beerAR.xlsx\u0026#34;,headeRTRUE,sheetIndex=1) dataset=as.matrix(beer) # Structure str(dataset) set.seed = 220 # Setting seed associa_rules = apriori(data = dataset, parameter = list(support = 0.3, confidence = 0.5)) #-------------------------------------------------------- rules \u0026lt;- associa_rules[size(lhs(associa_rules)) \u0026gt; 0] #-------------------------------------------------------- # Plot #itemFrequencyPlot(dataset, topN = 5) # Visualising the results inspect(sort(rules, by = \u0026#39;confidence\u0026#39;)) plot(rules, method = \u0026#34;graph\u0026#34;, measure = \u0026#34;confidence\u0026#34;, shading = \u0026#34;lift\u0026#34;) 關聯法則 Python程式碼(請自行確認是否能運作) # 匯入所需的函式庫 import pandas as pd from mlxtend.frequent_patterns import apriori, association_rules import matplotlib.pyplot as plt from openpyxl import load_workbook # 設定工作目錄 import os os.chdir(\u0026#34;c:/Koong/csv/DataScience\u0026#34;) # 讀取 Excel 檔案 beer = pd.read_excel(\u0026#34;beerAR.xlsx\u0026#34;, sheet_name=0) # 查看資料結構 print(beer.head()) # 設定隨機種子 np.random.seed(220) # 使用 apriori 算法進行關聯規則挖掘 frequent_itemsets = apriori(beer, min_support=0.3, use_colnames=True) # 計算關聯規則 rules = association_rules(frequent_itemsets, metric=\u0026#34;confidence\u0026#34;, min_threshold=0.5) # 過濾規則，僅保留左手邊 (lhs) 非空的規則 rules = rules[rules[\u0026#39;antecedents\u0026#39;].apply(lambda x: len(x) \u0026gt; 0)] # 依據信心度排序並顯示規則 sorted_rules = rules.sort_values(by=\u0026#39;confidence\u0026#39;, ascending=False) print(sorted_rules) # 繪製規則 import networkx as nx import matplotlib.pyplot as plt def plot_association_rules(rules): G = nx.DiGraph() for _, rule in rules.iterrows(): for antecedent in rule[\u0026#39;antecedents\u0026#39;]: for consequent in rule[\u0026#39;consequents\u0026#39;]: G.add_edge(antecedent, consequent, weight=rule[\u0026#39;confidence\u0026#39;]) pos = nx.spring_layout(G) plt.figure(figsize=(12, 8)) nx.draw(G, pos, with_labels=True, node_size=3000, node_coloR\u0026#34;skyblue\u0026#34;, font_size=10, font_coloR\u0026#34;black\u0026#34;, font_weight=\u0026#34;bold\u0026#34;, edge_coloR\u0026#34;grey\u0026#34;) plt.title(\u0026#34;Association Rules Graph\u0026#34;) plt.show() plot_association_rules(sorted_rules) 20240511 Decision Tree Azure Machine Learning Studio (https://studio.azureml.net ) (大數據分析_2023.pptx) CsutomerLifetimeValue DataSet (CustomerValueClean.csv)\nDecision Tree R語言程式碼 library(C50) library(gmodels) library(party) library(RColorBrewer) require(caret) require(e1071) setwd(\u0026#34;c://Koong//csv//DataScience//DSMK\u0026#34;) credit \u0026lt;- read.csv(\u0026#34;CustomerValueClean.csv\u0026#34;) #============== 產生 訓練資料及測試資料集 ================= # Create the training and test datasets set.seed(100) # Step 1: Get row numbers for the training data trainRowNumbers \u0026lt;- createDataPartition(credit$Response, p=0.7, list=FALSE) # Step 2: Create the training dataset trainData \u0026lt;- credit[trainRowNumbers,] # Step 3: Create the test dataset testData \u0026lt;- credit[-trainRowNumbers,] # Store X (自變數) and Y (依變數)for later use. x = trainData[, -3] y = trainData$Response #-------------------------------------------------------------------- #我們避免模型過度擬合(overfitting)，故要利用K-fold Cross-Validation# #的方法進行交叉驗證，我們使用caret這個套件，而K先設定為10次~ #---------------------------------- rpart + CARET ---------------- modelLookup(\u0026#34;rpart\u0026#34;) train_control \u0026lt;- trainControl(method=\u0026#34;cv\u0026#34;, numbeR3) train_control.model \u0026lt;- train(Response~., data=trainData, method=\u0026#34;rpart\u0026#34;, trControl=train_control, tuneLength = 6) train_control.model #--------- Prediction --------------------------------- pred \u0026lt;- predict(train_control.model, newdata=testData) # 用table看預測的情況 table(real=testData$Response, predict=pred) # 計算預測準確率 = 對角線的數量/總數量 confus.matrix \u0026lt;- table(real=testData$Response, predict=pred) sum(diag(confus.matrix))/sum(confus.matrix) # 對角線的數量/總數量 library(rattle) fancyRpartPlot(train_control.model$finalModel) #---------------------------- Advance CARET ------------------- #names(getModelInfo()) #modelLookup(\u0026#34;ctree2\u0026#34;) #---------------------------------- ctree2 + CARET ---------------- library(party) set.seed(123) train_control.model \u0026lt;- train( Response~., data = trainData, method = \u0026#34;ctree2\u0026#34;, trControl = trainControl(\u0026#34;cv\u0026#34;, number = 5), tuneGrid = expand.grid(maxdepth =3, mincriterion = 0.95 ) #tuneLength = 6, ) plot(train_control.model$finalModel) #--------- Prediction --------------------------------- pred \u0026lt;- predict(train_control.model, newdata=testData) # 用table看預測的情況 table(real=testData$Response, predict=pred) # 計算預測準確率 = 對角線的數量/總數量 confus.matrix \u0026lt;- table(real=testData$Response, predict=pred) sum(diag(confus.matrix))/sum(confus.matrix) # 對角線的數量/總數量 Decision Tree Python程式碼(請自行確認是否能運作) # 匯入所需的函式庫 import pandas as pd import numpy as np from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import confusion_matrix, accuracy_score from sklearn.ensemble import RandomForestClassifier import matplotlib.pyplot as plt import seaborn as sns import graphviz from sklearn.tree import export_graphviz from sklearn.tree import plot_tree # 設定工作目錄 import os os.chdir(\u0026#34;c://Koong//csv//DataScience//DSMK\u0026#34;) # 讀取 csv 檔案 credit = pd.read_csv(\u0026#34;CustomerValueClean.csv\u0026#34;) # 設定隨機種子 np.random.seed(100) # 分割訓練資料與測試資料 trainData, testData = train_test_split(credit, test_size=0.3, stratify=credit[\u0026#39;Response\u0026#39;], random_state=100) # 提取自變數 (X) 和依變數 (y) X_train = trainData.drop(columns=[\u0026#39;Response\u0026#39;]) y_train = trainData[\u0026#39;Response\u0026#39;] X_test = testData.drop(columns=[\u0026#39;Response\u0026#39;]) y_test = testData[\u0026#39;Response\u0026#39;] # 使用 K-fold 交叉驗證方法避免過度擬合 (overfitting) # 使用決策樹進行分類 dt_model = DecisionTreeClassifier(random_state=100) param_grid = {\u0026#39;max_depth\u0026#39;: range(1, 10), \u0026#39;min_samples_split\u0026#39;: range(2, 10)} # 設置交叉驗證 cv = GridSearchCV(dt_model, param_grid, cv=3) cv.fit(X_train, y_train) # 最佳模型 best_model = cv.best_estimator_ print(best_model) # 預測 y_pred = best_model.predict(X_test) # 用混淆矩陣查看預測結果 conf_matrix = confusion_matrix(y_test, y_pred) print(conf_matrix) # 計算準確率 accuracy = accuracy_score(y_test, y_pred) print(f\u0026#34;Accuracy: {accuracy:.4f}\u0026#34;) # 繪製決策樹 plt.figure(figsize=(20,10)) plot_tree(best_model, feature_names=X_train.columns, class_names=[\u0026#39;0\u0026#39;, \u0026#39;1\u0026#39;], filled=True) plt.show() # 使用更進階的決策樹模型 (ctree2) rf_model = RandomForestClassifier(random_state=123, max_depth=3, min_samples_split=2) rf_model.fit(X_train, y_train) # 預測 y_pred_rf = rf_model.predict(X_test) # 用混淆矩陣查看預測結果 conf_matrix_rf = confusion_matrix(y_test, y_pred_rf) print(conf_matrix_rf) # 計算準確率 accuracy_rf = accuracy_score(y_test, y_pred_rf) print(f\u0026#34;Accuracy (Random Forest): {accuracy_rf:.4f}\u0026#34;) 20240605 SEO 流量學SEO(06/05) SEO搜尋引擎最佳化 0.Google Search Engin 個案討論 1.SEO搜尋引擎最佳化實務驟 2.SEO 關鍵字工作表 4.目標關鍵字 *** SEO Certificate https://www.semrush.com/academy/courses/seo-principles-beginners-guide/ "},{"section":"Blog","slug":"/blog/2024/association_rule/","title":"association_rule","description":"123","date":"July 9, 2024","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/article_picture/association_rule_hu8405b666fc58af13d03d3c518c02e04e_372540_420x0_resize_q80_h2_lanczos.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"420\"\n          height=\"280\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/article_picture\\/association_rule_hu8405b666fc58af13d03d3c518c02e04e_372540_420x0_resize_q80_lanczos.jpg';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","imageSM":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n          \n          \n          \n          \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/article_picture/association_rule_hu8405b666fc58af13d03d3c518c02e04e_372540_100x100_fill_q80_h2_lanczos_smart1.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"100\"\n          height=\"100\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/article_picture\\/association_rule_hu8405b666fc58af13d03d3c518c02e04e_372540_100x100_fill_q80_lanczos_smart1.jpg';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","searchKeyword":"","categories":"marketing, dataAnalytics","tags":"marketing, dataAnalytics, 2024","content":"關聯分析法則: 關聯分析法則的目標是找出大型資料集中經常共同出現的物件組合，以及這個組合中物件的關聯性。 內容範例 先計算每個商品被購買的比率，以研究熱門暢銷的商品如何組合會更好。\nJuice：3/5 Soda：4/5 Detergent：2/5 Milk：1/5 WashingPowder：1/5 Film：1/5 Craker：1/5 篩除購買率低的商品 最小支持度(Minimum Support)：假設 30%(購買率只有 1/5 的商品被淘汰)，剩 Juice、Soda、Detergent。\n計算 Juice、Soda、Detergent 被同時購買的比率。\n同時買 Juice、Soda：2/5 同時買 Juice、Detergent：2/5 同時買 Soda、Detergent：1/5 計算信賴區間(Minimum Confidence)。\nJuice →Soda(買 Juice 的人中買 Soda 的比率)：2/3 Soda→ Juice：1/2 (50%) Juice → Detergent：2/3 (67%) Detergent→ Juice：2/2 (100%)為兩個關聯性最高的商品 This browser does not support PDFs. Please download the PDF to view it: Download PDF.\r"},{"section":"Blog","slug":"/blog/2024/data_analytic/","title":"data_Analytics","description":"123","date":"June 16, 2024","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/article_picture/dataAnalytics_hubd510a47a29ee5300d2a8fbe6dddf091_571474_420x0_resize_q80_h2_lanczos.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"420\"\n          height=\"280\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/article_picture\\/dataAnalytics_hubd510a47a29ee5300d2a8fbe6dddf091_571474_420x0_resize_q80_lanczos.jpg';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","imageSM":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n          \n          \n          \n          \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/article_picture/dataAnalytics_hubd510a47a29ee5300d2a8fbe6dddf091_571474_100x100_fill_q80_h2_lanczos_smart1.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"100\"\n          height=\"100\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/article_picture\\/dataAnalytics_hubd510a47a29ee5300d2a8fbe6dddf091_571474_100x100_fill_q80_lanczos_smart1.jpg';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","searchKeyword":"","categories":"Application, DataAnalytics","tags":"Application, DataAnalytics, 2024","content":"巨量資料分析: 巨量資料分析是指用來從不同的大量高速資料集收集、處理和取得深入解析的方法、工具和應用程式 第一段 pnorm() 算常態分布的機率 雖說bigData 但我們通常都只有partial data\nautoML 套件 取代 data analyst\ndata:\nsingal: patterns insight noise AI崛起原因\nAdvanced of learning algorithms 演算法的進步 exponential growth of data(Big data) cheaper computation deep learning 向下走 原因:不太能推論出因果關係\nsmart apps:會蒐集使用者資料 回傳server 調整模型 提供更好的服務\n第二段 big data 從哪來?\ntraditional data(tabular data) :類似關聯式資料庫內的資料 (1) continuous data:連續型資料 e.g.收入 (2) categorical data:間段型資料 e.g.男女\nimage data: 圖資料 voice data: text data network data: 比較大的graph 例 social network 可以用adjacency matrix 表示(研究所考試考過) 資料的結構 結構化 半結構化(key-value):json xml 非結構化:聲音 圖片 email etc.\ndescriptive: pie chart 之類的 diagnostic: 假設檢定 因果關係 predictive: 迴歸 prescriptive: 最佳化決定\nml model: 探討變數關係 simulation model: 定義不同物件 然後跑模擬\napply()\nlapply(): lapply 的 l 代表 list，也就是透過 lapply 函數操作完之後，會回傳一個 list\nsapply(): sapply 的 s 代表 simple，意思是透過函數 sapply 回傳的結果是將 list 形式簡單化 (simplified) 後的 vector\nmapply(): mapply 的 m 指的則是 multivariate，意思是可以同時使用多個變數\nsd(): 標準差\ndatapipeline 資料科學家的工作 particle sworm optimization :algo 的一種 可以上netlogo web試用\n想想看: 為甚麼鴿子不會撞在一起?\n\u0026lsquo;\u0026lsquo;用來框vector \u0026ldquo;\u0026ldquo;用來框string\n預測要先夠準 再去interpretation\nR的資料集: mtcars R: lm() linear model : learner:要學的東西 model:做出來的東西 lime interpreter multi-modal learning 多模態 (學習用人的想法去學習): 把不同類型的資料混合使用\nshortcut learning 從我們非預期的地方學到如何判斷結果\ne.g.有尺就是皮膚癌 correlation alone doesnt imply causation\nconfounder 干擾因子 randomized controlled trial = AB test simpson paradox survivor bias\nfederate learning 只訓練模型丟到雲端 不需要一些較隱私的data\nr sql語法 multi-label learning 多標籤學習 同一種東西可能屬於很多類型 e.g.一部電影 可能同時是恐怖電影與愛情電影 multi-tasking 不同domain (e.g.年齡 教育程度) multi-label 同domain binary class 一種東西只會有兩種分類 是貓還是狗 multi class 一種東西可能是a b c\n模型可以\n預測 prediction 解釋變數間關係 interpretation 最佳化 optimization horizontal join: 以第一column做合併 vertical join:以第一row做合併 vertical join操作:\nunion diff intersection r的最小單位是vector 不是scalar\n因變數取在~符號的左邊，右邊由解釋變數組成。下面是一個示例公式，可以說明~符號的使用。 s \u0026lt;- lhs ~ rhs s lhs ~ rhs\nI() identity 建立一個變數 e.g. I(a+b) 建立一個變數=a+b\ni.i.d idependent identical distribution\no.o.d out of distribution\ndata granularity 資料顆粒度 continuous \u0026ndash;\u0026gt;categorize\u0026ndash;\u0026gt;category data reduction 資料縮減分類:\ndimensionality reduction 維度縮減 國英數自 國英-\u0026gt;語文 數自-\u0026gt;數理 numerousity reduction:把數字做離散 R資料結構: scalar vector matrix tensor\ndata augmentation: problem:如果用linear model可能造成collinearity sol:用tree之類的方法就不會collinearity了\ndata modeling algorithm modeling\n建模型的用處:\nprediction (accurarcy 要夠準) intepretation(合不合理 算命仙) optimization There is no neccesary connection between model acuuracy and model interpretability/complexity\nRashomon sets (羅生門集): 不同算法都得出相同結果 accuracy也相近-\u0026gt;選最好解釋的(parsiomous model)\nparsiomous model: 參數不多也好解釋的\nrule learning 解釋器-幫模型標出來模型是怎麼判斷的\nOOD-out of distribution\nunderstanding data 步驟:\nUnivariate analysis Bivariate analysis Multivariate analysis zero variant = near zero variant:變數沒有變異\nUnivariate Analysis\nhow to handle missing value?\ncomplete cases 缺的就刪掉 data imputation 算linear model 然後推算 continuous data補值: 可以用missing indicator 補0/1 告訴模型是不是捕來的 categorical data補值:\nmerge new level feature selection 選適合的變數放進模型\nbivariate anaysis\ntwo sample T-test:拿來算兩種不同類型的變數 x1 x2 e.g. cont. v.s cat.\n一個categorical:one way ANOVA 兩個categorical:two way ANOVA chi-square:檢定兩個categorical 男女 學歷\n今天剛好在查詢各種檢定運用的情境，就稍微在這裡整理一下： t test: 主要是用在比較兩組之間“平均值”是否有差異（只能比較兩組）。\nTwo sample t-test：兩組之間彼此獨立，比如牛吃a牌飼料跟b牌飼料的產乳量有沒有不同。 最多只能用a b 兩種類別，再多就不能用了\nPaired Sample t-test：兩組之間相互關聯，比如一年前後相同一批學生的數學成績有無差別。\nANOVA: categorical(x) 對 continuous(y) 與t-test做相同的事情，但是可同時比較多組。\nOne-way Anova：自變數只有一個，比如性別對於智力是否影響。\nTwo-way Anova：自變數有多個，比如性別與種族對於智力是否影響。\none sample t-test univariate two sample t-test bivariate paired t-test bivariate\nstatistical learning:\ngeneral linear model: anova ancova SLR(simple linear regression) generalized linear model:logistic regression poisson regression ML CART Rule-learning DNN (KNN,SVM) bayesian network regular experssion (Regex)\nscore = 20+10 * hrs local interpretation解釋一筆資料如何作出決定:因為公式長這樣 20+10* hrs= score global intepretation解釋現象:讀越多書分數越高\ndiscrete:\n轉成百分比-\u0026gt;密度函數 prob=density 舉例: head 3次 tail 7次 -\u0026gt; head 0.3 tail 0.7 continuous: prob = density* range(範圍) density func: 160cm-170cm cumulative func: 150以下 quantile func(inverse CDF):給一個機率 求點是多少\nd:密度函數 p:累積密度函數 cumulative q:quantile pdf-\u0026gt;continuous pmf-\u0026gt;discrete\nchi-square: goodness of fit test 檢驗兩個categorical\ndistance 有symmetric的特性 e.g. 台北-高雄 = 高雄-台北\ndivergence 衡量分布的變異度哪個比較大 e.g. d(a,b) != d(b,a) KL divergent:衡量系統是否開始不穩定 Shapiro-Wilk 常態性檢定: 檢定連續型樣本是否屬於常態分配 Kolmogorov-Smirnov 常態性檢定:檢定連續型樣本是否屬於常態分配 K-W test: 用中位數檢定樣本 fisher-exact test:every cell in crosstab\u0026lt;5 可分析兩組類別categorical資料之間是否有顯著相關 Chi-squared goodness of fit test:檢定 discrete的樣本是否屬於常態分配 chi-square test of independence: 有母數:假設母體有一種分配 無母數:假設母體沒有特定分配\n現在做資料分析:\nquick and dirty:做資料先簡單處理，就丟進去跑，不要花一堆時間整理資料 incrementally interatively accuracy 準確 precision 精確\ny proxy:可以當成y的其他變數(類似proxy variable)\nmodel architecture design: good model要素:\ndeep(nested): (1)一個模型的output為另一個模型的input (2)越來越接近資料 (3)prevent overfitting diverse 從不同角度看 interpretable generative causal mutual information(in bit): how much info in x about y\ndata processing inequality(DPI): 資料經過轉換之後，Mututal information(越來越高)\n相關係數:\nPearson Spearman Kendall 相關係數在現實生活效用不高: 例如curve 相關係數就趨近於0 KNN:\nlazy algo 類比學派 KNN for regression 從a 點附近兩個點平均後去推測a點 continuous要做feature rescaling 讓尺度要一樣 不然會有bias continuous variable 為scale sensitive 所以要做rescaling rescaling 方法:\n標準化 min max standarization 做rescaling 時都要做紀錄，看是怎麼轉換的 traing-test splittng原則: model至少要能train起來，至少要能testing training set:train/select best fit model (選最適合的model) testing set:estimate model generalization\ndata preprocessing object: package: caret\n類比學派:\n你就是相似的你-\u0026gt;從目標去推測目標(物以類聚) Euclidean distance Manhattan distance lazy algo:推論階段才去建模 資料漂移 (Data drift) 指的是從訓練到服務之間的資料改變，也就是特徵的統計性質改變 (資料分佈 X 改變)，像是現在的房子越蓋越小。\n概念漂移 (Concept drift) 指的則是世界改變使得 Ground Truth 改變，也就是標籤的統計性質改變 (映射 X → Y 改變)，像是炒房使得同樣大小的房子價格改變。\nR-square:只是針對training set沒有針對testing set 所以盡量不要用\ncategorical rescaling: 要有encoding encoding方法:\none-hot 非常佔記憶體(差) dummy coding(差) frequency encoding 用機率來記 用numeric來記:embedding target encoding:將資料壓縮成你要的形式 categorical encoder:\nsupervised unsupervised (one hot) deep learning不一定只能用ANN做 也可以用tree之類的做 (但linear model不行)\n資料轉換要包含 linear \u0026amp;非linear才是好的 linear model (linearmodel)-\u0026gt;還是lnear model\nfeature rescaling-\u0026gt;針對continuous linear model 也是scale sensitive\n基於規則的都有-\u0026gt;scale sensitive 非基於規則-\u0026gt;沒有scale sensitive (e.g.tree xgboost)\nindicator variable: 變數1就是有0就是沒有 (binary)\n上圖跑出來的係數是負的:原因是因為data是成對(paired)的(同一人每5年做一次)，應該要每個人都畫一條regression 係數才是正的(符合邏輯) sol:跑paired t-test wilcox.test, 兩點做相減再跑回歸\n資料跟資料有關係: 統計: correlated ML: network 成對資料可以用 mixed effect model 來衡量: mixed effect = fixed +random fixed effect: 隨著時間不會改變(如id對性別) random effect: 隨著時間會改變(如id對體重)\nmixed effect model:\nmixed effect linear model GEE 廣義估計方程 mixed effect neural net mixed effect random forest paper類型:\ntutorial-like (introducting):頁數很多 很像教科書的概念 survey(review): 2-column, \u0026lt;15頁:介紹已經有的概念、讓已經在該領域做研究的人快速了解該領域的發展 viewpoint: 2-column, \u0026lt;5頁 ,多是頂尖期刊才會有 regular: 2-column, 8-10(IEEE),(重視novelty) 如何判斷paper值不值得看:\n只看跟自己研究有關係的 盡量不要看超過10年的paper 找不到paper的pdf可以用 scihub.se 把網址貼上去 看paper順序: abstract-\u0026gt; conclusion-\u0026gt; intro deep learning三巨頭: bengio, lecun, geoffrey hinton\n跟資料有關係，跟模型沒關係 low noise low signal常發生情況: time series high signal low noise常發生情況:\n來自實驗室環境或是工廠 資料裡面就有直接寫出來了 e.g. 預測敗血症 結果病例裡面就有敗血症 如果同一人衡量超過2次(非成對、或是每人衡量次數不一樣e.g.過世、衡量的時間點或間隔不一樣)就不能用paired\n模型複雜度跟預測準度沒關係 overfitting 就是誤把noise當成signal\n如果資料及不符合參數假設(e.g.常態分配) -\u0026gt;可以用Wilcoxon rank-sum test (WRS)\nCausal effect 可以用\npotential outcome framework Rubin causal model cross validation 可以用來驗證training vs testing\nextrapolation: 可以在DB裡面加constraints 來解決 e.g.mpg~ weight ,mpg\u0026gt;0\nMulti-tasking learning:可以用同樣的資料訓練不同類型模型，幫助加強模型準度 e.g.預測人的年齡，可以先預測性別\nPSO particle swarm optimization: 可以用來找最佳化的配置 k-fold Cross validation 隨機分成不同part k=10 會train 10 個model k=20 會train 20 個model 文獻證明: 10-20個fold較適合，至少要有5個fold\nEnsemble machine learning :is a kind of modeling technique that promotes deep and diverse models by using different learners and aggregating multiple models to predict the outcome.\n一般Linear model 通常不太會overfit 除非signal比較複雜\ncross validation 是可以協助挑選好的模型架構，比對overfit 或是underfit，不能直接提升精準度\n用y proxy 去預測y noise低\u0026amp;signal高-\u0026gt; 沒意義\ntime series:通常signal low noise也low\n如果validation error 跟testing error差很多，模型就有問題\nconcept drift 步驟\ndetection understanding adaption (1)model retrain (2)model parameter update (3)model redesign confirmatory data analysis:驗證性分析 確認x影響y的關係 e.g.肺癌 = smoke +gender+age 如果把smoke拿掉非常不像話，所以要做confirmatory analysis，確定x影響y的關係 smoke 叫做voi (variable of interest)\nexploratory data analysis 探索性分析 就是沒有voi 的confirmatory data analysis\nfeature selection: 挑模型套件 R: CARET Python: mltax\n模型數量= 2^k-1個模型 human(expert) in the loop ML:專家參與模型建立\nFeature selection 分為:\nbackward selection: 1.探索性分析比較適合 2.一個一個刪 刪到error開始上升就停了 3.\nforward selection: (老師幾乎都用這個) 1.驗證性分析比較適合 2.又稱為RFE recursive feature engineering 3.又稱為 forced-in 一次只加一個變數進去，直到放變數進去error不會再下降\nhybrid selection : 1.先用forward 再用 backward 2.第一線人員常用 3.可以結合forward backward 的優點\nranking of variable importance: 1.排出來哪個變數的重要性比較高 2.最好跟domain expert討論\nlinear model排出來的常常是不準的，因為會有共線性的問題\nlasso regression: 就是linear model 後面加一個penalty\nlogistic func 是把 regression 壓成機率介於0-1之間 odds ratio 介於0-無限大 odds ratio 只要大於1 就可以說有關係 小於一 比較沒關係\n時間序列: 類型: 平穩型stationary 無定向型 drifting 趨勢型 trend 季節型 seasonality 外部影響型exogenous 特徵: 滯後性lag 週期性periodicity 趨勢性trend\nlogistic regression 採用MLE來計算機率 epiDisplay:用來產生公衛 商用報表(adjusted odds ratio比較表)\n對glm的coefficient直接取自然對數 就可以得到adjusted OR\nclass imbalance: 定義:因為資料預測超級不平衡，導致accuracy超高，但實際上沒意義 e.g.預測x實際也錯x solution:\nopitmal cutoff: 調整thrshold但可能錯殺無辜(把想知道的東西刪掉)\nresampling: 分為up-sampling\u0026amp;down-sampling，分為小部分的資料，建模後再合併在一起 model Architecture Redesign: one-class,cost-sensitive learning\nAUC介於0-1之間 越接近1，classifieer越好 實務上 (\u0026gt;0.9)要覺得too good to be true (\u0026gt;0.7-0.9) 這個範圍還不錯\nAUC 越往左上越好\n讓roc auc平衡:用youden\u0026rsquo;s J 取得youden\u0026rsquo;s J 算法-\u0026gt; J= sensitivity+specificity-1\nglm(x,data,family) 如果要產出logistic regression family = binomial e.g. gl,(default~balance,data=Default, family = \u0026lsquo;binomial\u0026rsquo;)\nepiDisplay 套件可以把logistic regression 產出一個表 threshold 上升 sensitivity 下降 Speficity下降 trees spliting 代表把tree分支成很多部分 tree的over-spliting = regression 的overfitting\nmodel-based tree: 定義:先用tree把資料切開來，再用linear model等方法預測\n越靠近root的變數越重要 因為是被優先挑出來的\nCART 又叫做 recursive partitioning\ntree pruning 限制tree的生長 分為\npre-prunning:限制切出來的區塊至少要有_%的數量才能畫出來 post-pruning: (1)cost-complexity (2)cost-\u0026gt;MR 錯誤率 decision stump:只有一個分支的tree\nrpart 可以拿來算 tree剪枝要剪多少 tree 的 優點:\n好解釋 跟尺度無關 不用編碼 缺點:\n不準 因為都是用2分法然後取平均數預測 expressive power不好 因為現實生活中不是每個都if then的問題 teacher student model 又稱為 knowledge distillation\nBootrapping: 特點: row-sampling bootrap aggregation (bagging) 流程:先切不同資料-\u0026gt;建不同樹-\u0026gt;得到不同y-head-\u0026gt;再用3個y-head的平均數去預測 實務上:\n64-128個tree會比較準 r的套件預設randomforest的tree要長500棵 盡量先overfitting 先降低bias 再處理variance 把各個模型做平均 variance就會變小 OOB(out of bag) data:可以拿來當validation data 大部分都是有1/3做validation set bagging tree最大的缺點就是幾乎都長得很像，因為重要的變數不管在哪一棵樹幾乎都是一樣的。 解決辦法:故意把重要變數遮住讓樹去長-\u0026gt; 也就是randomforest 隨機森林: 缺點:變準了，但也變得難解釋了\n連結 "}]